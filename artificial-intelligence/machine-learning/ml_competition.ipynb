{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Ni4DGXAdem"
      },
      "source": [
        "# Importing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72f-6W1t9I0Y"
      },
      "outputs": [],
      "source": [
        "# Getting the files ready\n",
        "!rm -rf *\n",
        "!wget ai-unibuc.zip\n",
        "!unzip -q ai-unibuc.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kbq7pzlVBDx_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "from skimage import io\n",
        "from sklearn import preprocessing, svm, metrics\n",
        "\n",
        "# The dimension of the images (in case you don't resize them, keep it at (50, 50))\n",
        "dim = (50, 50)\n",
        "\n",
        "# Denoise the image using a function from CV2\n",
        "def denoise_image(image):\n",
        "    return cv2.fastNlMeansDenoising(image, None)\n",
        "\n",
        "# A function that takes an image and returns the image resized\n",
        "def resize_image(image):\n",
        "    return cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "# A function that takes an image and modifies the saturation and brightness\n",
        "def adjust_image(image, alpha = 1.95, beta = 0):\n",
        "    return cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
        "\n",
        "# Preprocessing of an image, you can comment the functions you don't want to run\n",
        "def preprocess(image): \n",
        "    # image = denoise_image(image)\n",
        "    # image = resize_image(image)\n",
        "    # image = adjust_image(image)\n",
        "    return image\n",
        "\n",
        "# Load the train and validation data\n",
        "def load_data(type):\n",
        "    lines = [line for line in open(f'{type}.txt')]\n",
        "    images = np.empty((len(lines), dim[0], dim[1]), 'float64')\n",
        "    labels = np.empty((len(lines)), dtype=\"int64\")\n",
        "    filenames = np.empty((len(lines)), dtype=\"<U16\")\n",
        "\n",
        "    for index, line in enumerate(lines):\n",
        "        filename, label = line.split(',')\n",
        "        image = cv2.imread(os.path.join(type, filename), cv2.IMREAD_GRAYSCALE)\n",
        "        images[index] = preprocess(image)\n",
        "        filenames[index] = filename\n",
        "        labels[index] = int(label.replace('\\n', ''))\n",
        "\n",
        "    return images, labels, filenames\n",
        "    # return labels, filenames\n",
        "\n",
        "# Load the test data\n",
        "def load_test():\n",
        "    lines = [line for line in open('test.txt')]\n",
        "    images = np.empty((len(lines), dim[0], dim[1]), 'float32')\n",
        "    filenames = []\n",
        "    \n",
        "    for index, line in enumerate(lines):\n",
        "        filename, _ = line.split('\\n')\n",
        "        image = cv2.imread(os.path.join('test', filename), cv2.IMREAD_GRAYSCALE)\n",
        "        images[index] = preprocess(image)\n",
        "        filenames.append(line.replace('\\n', ''))\n",
        "    return images, filenames\n",
        "    # return filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6h-r_RREIqh"
      },
      "outputs": [],
      "source": [
        "# Loading the data\n",
        "train_images, train_labels, train_files = load_data('train')\n",
        "validation_images, validation_labels, validation_files = load_data('validation')\n",
        "test_images, test_files = load_test()\n",
        "# train_labels, train_files = load_data('train')\n",
        "# validation_labels, validation_files = load_data('validation')\n",
        "# test_files = load_test()\n",
        "\n",
        "# print (train_images)\n",
        "# print (train_labels)\n",
        "\n",
        "# If we loaded it into the memory we make sure everything is ok\n",
        "\n",
        "io.imshow(cv2.imread(os.path.join('./train', train_files[0])))\n",
        "io.show()\n",
        "\n",
        "io.imshow(train_images[0].astype(np.uint8))\n",
        "io.show()\n",
        "\n",
        "# For everything except transfer learning we normalise the data and reshape it\n",
        "train_images /= 255\n",
        "validation_images /= 255\n",
        "test_images /= 255\n",
        "\n",
        "train_images_reshaped = train_images.reshape((train_images.shape[0], train_images.shape[1] * train_images.shape[2]))\n",
        "validation_images_reshaped = validation_images.reshape((validation_images.shape[0], validation_images.shape[1] * validation_images.shape[2]))\n",
        "test_images_reshaped = test_images.reshape((test_images.shape[0], test_images.shape[1] * test_images.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1eVNVPq4nOm"
      },
      "outputs": [],
      "source": [
        "# Optional: if we don't load the data in memory, we move it in folder to use dataGenerators\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "for label in range(0, 3):\n",
        "    if not os.path.exists(os.path.join(cwd, f'train/class_{label}')):\n",
        "        os.mkdir(os.path.join(cwd, f'train/class_{label}'))\n",
        "    if not os.path.exists(os.path.join(cwd, f'validation/class_{label}')):\n",
        "        os.mkdir(os.path.join(cwd, f'validation/class_{label}'))\n",
        "if not os.path.exists(os.path.join(cwd, f'test/test_folder')):\n",
        "    os.mkdir(os.path.join(cwd, f'test/test_folder'))\n",
        "\n",
        "for label, filename in zip(train_labels, train_files):\n",
        "    os.rename(os.path.join(cwd, f'train/{filename}'),\n",
        "              os.path.join(cwd, f'train/class_{label}/{filename}'))\n",
        "\n",
        "\n",
        "for label, filename in zip(validation_labels, validation_files):\n",
        "    os.rename(os.path.join(cwd, f'validation/{filename}'),\n",
        "              os.path.join(cwd, f'validation/class_{label}/{filename}'))\n",
        "\n",
        "for filename in test_files:\n",
        "    os.rename(os.path.join(cwd, f'test/{filename}'),\n",
        "              os.path.join(cwd, f'test/test_folder/{filename}'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aeb5h7XB4qZ8"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_generator =  ImageDataGenerator(rescale= 1./255).flow_from_directory(\n",
        "    './train/',\n",
        "    batch_size=32,\n",
        "    color_mode='rgb',\n",
        "    target_size=dim\n",
        ")\n",
        "\n",
        "validation_generator = ImageDataGenerator(rescale=1. / 255).flow_from_directory(\n",
        "    './validation/',\n",
        "    batch_size=32,\n",
        "    color_mode='rgb',\n",
        "    target_size=dim\n",
        ")\n",
        "\n",
        "test_generator = ImageDataGenerator(rescale=1. / 255).flow_from_directory(\n",
        "    './test/',\n",
        "    shuffle=False,\n",
        "    classes=None,\n",
        "    color_mode='rgb',\n",
        "    target_size=dim\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wZX48qCPXY7"
      },
      "source": [
        "# NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlWX3Wy4Z9AP"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# These two functions are taken from the second laboratory\n",
        "def interval_calculator(num_bins):\n",
        "    return np.linspace(start=0, stop=1, num=num_bins)\n",
        "\n",
        "def value_to_bins(x, bins):\n",
        "    x_to_bins = np.digitize(x, bins, right=True)\n",
        "    return x_to_bins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLD3QtT8MaoS"
      },
      "outputs": [],
      "source": [
        "num_bins = 9\n",
        "bins = interval_calculator(num_bins)\n",
        "train_images_bins = value_to_bins(train_images_reshaped, bins)\n",
        "\n",
        "naive_bayes_model = MultinomialNB()\n",
        "naive_bayes_model.fit(train_images_bins, train_labels)\n",
        "\n",
        "validation_images_bins = value_to_bins(validation_images_reshaped, bins)\n",
        "naive_bayes_model.predict(validation_images_bins)\n",
        "print (f'Validation score = {naive_bayes_model.score(validation_images_bins, validation_labels)}')\n",
        "\n",
        "test_images_bins = value_to_bins(test_images_reshaped, bins)\n",
        "predictions = naive_bayes_model.predict(test_images_bins)\n",
        "\n",
        "with open('submission.txt', 'w') as file:\n",
        "    file.write(f'id,label\\n')\n",
        "    for elem in zip(test_files, predictions):\n",
        "        file.write(f'{elem[0]},{elem[1]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eah64VL0S7S0"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RizeO7oGTBqR"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors = 11, metric = 'l2', n_jobs = -1)\n",
        "# knn_classifier = KNeighborsClassifier(n_neighbors = 9, metric = 'l1', n_jobs = -1)\n",
        "knn_classifier.fit(train_images_reshaped, train_labels)\n",
        "\n",
        "knn_classifier.predict(validation_images_reshaped)\n",
        "print (f'Validation score = {knn_classifier.score(validation_images_reshaped, validation_labels)}')\n",
        "\n",
        "predictions = knn_classifier.predict(test_images_reshaped)\n",
        "with open('submission.txt', 'w') as file:\n",
        "    file.write(f'id,label\\n')\n",
        "    for elem in zip(test_files, predictions):\n",
        "        file.write(f'{elem[0]},{elem[1]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzmP0F6pfn4x"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzOASWjNgQQ-"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm, metrics\n",
        "\n",
        "clf = svm.SVC(C=3, kernel='rbf')\n",
        "clf.fit(train_images_reshaped, train_labels)\n",
        "print (f'Validation score = {metrics.accuracy_score(validation_labels, clf.predict(validation_images_reshaped))}')\n",
        "\n",
        "predictions = clf.predict(test_images_reshaped)\n",
        "with open('submission.txt', 'w') as file:\n",
        "    file.write(f'id,label\\n')\n",
        "    for elem in zip(test_files, predictions):\n",
        "        file.write(f'{elem[0]},{elem[1]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbk27uEo0yI_"
      },
      "source": [
        "# MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWqu-e_k1DV_"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,), \n",
        "                                activation='relu', \n",
        "                                solver='adam', \n",
        "                                alpha=0.05,\n",
        "                                max_iter=2000)\n",
        "mlp.fit(train_images_reshaped, train_labels)\n",
        "print (f'Validation score = {mlp_classifier.score(validation_images_reshaped, validation_labels)}')\n",
        "\n",
        "predictions = mlp.predict(test_images_reshaped)\n",
        "with open('submission.txt', 'w') as file:\n",
        "    file.write(f'id,label\\n')\n",
        "    for elem in zip(test_files, predictions):\n",
        "        file.write(f'{elem[0]},{elem[1]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsodC_99-sgK"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkFvhrPc8EeG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models, losses, regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_images_reshaped = train_images.reshape((-1, 50, 50, 1))\n",
        "validation_images_reshaped = validation_images.reshape((-1, 50, 50, 1))\n",
        "test_images_reshaped = test_images.reshape((-1, 50, 50, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kjYrYlL-97K"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(50, 50, 1)))\n",
        "model.add(layers.MaxPooling2D())\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D())\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D())\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(16))\n",
        "model.add(layers.Dense(3, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images_reshaped, train_labels, batch_size=32, epochs=100, \n",
        "                    validation_data=(validation_images_reshaped, validation_labels))\n",
        "\n",
        "predictions = model.predict_classes(test_images_reshaped)\n",
        "with open('submission.txt', 'w') as file:\n",
        "    file.write(f'id,label\\n')\n",
        "    for elem in zip(test_files, predictions):\n",
        "        file.write(f'{elem[0]},{elem[1]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfcLh_eV5ia0"
      },
      "source": [
        "# Transfer learning - Tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpTxbyXZ51kJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from keras import applications, layers, regularizers, optimizers, models\n",
        "from keras.applications.vgg16 import VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOPLICNs5nnB"
      },
      "outputs": [],
      "source": [
        "base_model = VGG16(include_top=False, input_shape=(224,224,3), pooling='avg')\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable =False\n",
        "\n",
        "x = layers.Flatten()(base_model.layers[-1].output)\n",
        "x = layers.Dense(3, kernel_regularizer=regularizers.l2(0.01),\n",
        "            activity_regularizer=regularizers.l1(0.01), activation='softmax')(x)\n",
        "model = models.Model(inputs = base_model.input, outputs = x)\n",
        "\n",
        "model.compile(optimizer=optimizers.SGD(learning_rate=1e-3),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator)\n",
        "probabilities = model.predict(test_images) \n",
        "predictions = probabilities.argmax(axis=-1)\n",
        "\n",
        "with open('submission.txt', 'w') as file:\n",
        "    file.write(f'id,label\\n')\n",
        "    for elem in zip(test_files, predictions):\n",
        "        file.write(f'{elem[0]},{elem[1]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IjZO5VInBx1"
      },
      "source": [
        "# Transfer learning - PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do02RIksXOSB"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install torchdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCaeotbRnRrE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from torch.optim import SGD\n",
        "from torch.nn import CrossEntropyLoss, Linear\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "from torchvision.models import resnet50, alexnet, vgg16\n",
        "\n",
        "from torchvision.transforms import Resize, ToTensor, Normalize, Compose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKsbrHrsaAwW"
      },
      "outputs": [],
      "source": [
        "def score(loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (images, labels) in loader:\n",
        "            x_input = images.to(device)\n",
        "            y_true = labels.to(device)\n",
        "            y_pred = torch.argmax(model(x_input), dim=1)\n",
        "            \n",
        "            total += y_pred.size(0)\n",
        "            correct += (y_pred == y_true).sum()\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "def predict_classes(loader):\n",
        "    y_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (images, labels) in loader:\n",
        "            x_input = images.to(device)\n",
        "\n",
        "            y_pred = model(x_input)\n",
        "\n",
        "            y_pred = torch.argmax(model(x_input), dim=1)\n",
        "            y_preds.extend(y_pred.tolist())\n",
        "\n",
        "    return y_preds\n",
        "\n",
        "def fit(train_loader, validation_loader, epochs):\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = 0.0\n",
        "        val_loss = 0.0\n",
        "        \n",
        "        for (images, labels) in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            x_train = images.to(device)\n",
        "            y_true = labels.to(device)\n",
        "\n",
        "            y_pred = model(x_train)\n",
        "\n",
        "            (loss_function(y_pred, y_true)).backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print (f'Epoch {epoch} | val_acc: {score(validation_loader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1oarPKwLzwy"
      },
      "outputs": [],
      "source": [
        "model = resnet50(pretrained=True)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "loss_function = CrossEntropyLoss()\n",
        "\n",
        "layers = [Resize(224), \n",
        "        ToTensor(), \n",
        "        Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]                                               \n",
        "\n",
        "transform = Compose(layers)\n",
        "\n",
        "optimizer = SGD(model.parameters(),\n",
        "                lr=1e-3,\n",
        "                momentum=0.9)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(ImageFolder(root='train/', transform=transform), batch_size=32, shuffle=True)\n",
        "validation_loader = DataLoader(ImageFolder(root='validation/', transform=transform), batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(ImageFolder(root='test/', transform=transform), batch_size=32, shuffle=False)\n",
        "\n",
        "# model.eval()\n",
        "# model.classifier[-1] = Linear(model.classifier[-1].in_features, 3)\n",
        "model.fc = Linear(model.fc.in_features, 3)\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "fit(train_loader, validation_loader, 2)\n",
        "\n",
        "y_pred = predict_classes(test_loader)\n",
        "\n",
        "with open('submission.txt', 'w') as file:\n",
        "    file.write(f'id,label\\n')\n",
        "    for elem in zip(sorted(test_files), y_pred):\n",
        "        file.write(f'{elem[0]},{elem[1]}\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "b_Ni4DGXAdem",
        "1wZX48qCPXY7",
        "eah64VL0S7S0",
        "jzmP0F6pfn4x",
        "rbk27uEo0yI_",
        "DsodC_99-sgK",
        "SRrqGdac67Ac",
        "kzMzHmUbnZmR",
        "FfcLh_eV5ia0",
        "9IjZO5VInBx1",
        "8XG0m_m_BUAz"
      ],
      "name": "Copy of Mitoi_È˜tefan-Daniel_ML_Combined.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
